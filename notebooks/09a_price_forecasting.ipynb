{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 09a: ML Price Forecasting — NO_5 (Bergen)\n",
    "\n",
    "**Phase 3** — Machine learning price forecasting using gradient boosted trees.\n",
    "\n",
    "This notebook predicts day-ahead electricity prices (EUR/MWh → NOK/kWh) for NO_5 (Bergen)\n",
    "using **fundamental features only**: weather, reservoir levels, gas prices, calendar patterns,\n",
    "load, and generation data.\n",
    "\n",
    "**Fundamentals-only approach:**\n",
    "We deliberately exclude price lag features (price_lag_1h, price_lag_24h, rolling means, etc.).\n",
    "These autoregressive features let the model learn \"price ≈ yesterday's price\" — a shortcut\n",
    "that masks the actual supply/demand drivers. By removing them, the model must learn from\n",
    "**production, consumption, reservoir levels, commodity prices, weather, and calendar** —\n",
    "the physical factors that actually determine electricity price.\n",
    "\n",
    "**Why ML over statistical methods?**\n",
    "Statistical methods (ARIMA, SARIMA, ETS) rely on price history alone. Tree-based models\n",
    "(XGBoost, LightGBM, CatBoost) can leverage ALL fundamental features simultaneously,\n",
    "capturing complex nonlinear relationships between weather, supply, demand, and price.\n",
    "\n",
    "**Methods:**\n",
    "1. Naive baseline (same hour last week) — the bar to beat\n",
    "2. Statistical baselines (best ARIMA/SARIMA — compressed summary)\n",
    "3. XGBoost\n",
    "4. LightGBM\n",
    "5. CatBoost\n",
    "6. Weighted ensemble (inverse-MAE)\n",
    "7. Walk-forward validation (6-fold)\n",
    "8. SHAP feature importance analysis\n",
    "9. Yr weather forecast integration (forward-looking predictions)\n",
    "\n",
    "**Data split:**\n",
    "- Training: 2022-01-01 to 2024-12-31 (~26,280 hours)\n",
    "- Validation: 2025-01-01 to 2025-06-30 (~4,344 hours)\n",
    "- Test: 2025-07-01 to 2026-02-22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 0. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "\n",
    "# Project imports\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from src.models.forecasters import NaiveForecaster\n",
    "from src.models.train import (\n",
    "    MLPriceForecaster,\n",
    "    prepare_ml_features,\n",
    "    walk_forward_validate,\n",
    "    train_ensemble,\n",
    "    forecast_with_yr,\n",
    ")\n",
    "from src.models.evaluate import compute_metrics, comparison_table, plot_forecast, plot_residuals\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(name)s %(levelname)s %(message)s\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 5)\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "ZONE = \"NO_5\"\n",
    "print(f\"Forecasting target: price_eur_mwh for {ZONE} (Bergen)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature matrix\n",
    "data_path = Path.cwd().parent / \"data\" / \"processed\" / \"features_NO_5_2022-01-01_2026-01-01.parquet\"\n",
    "df = pd.read_parquet(data_path)\n",
    "print(f\"Loaded: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "\n",
    "# Extract target\n",
    "target = df[\"price_eur_mwh\"]\n",
    "\n",
    "# Truncate at 2026-02-22 (end of available verified data)\n",
    "cutoff = pd.Timestamp(\"2026-02-22\", tz=\"Europe/Oslo\")\n",
    "df = df[df.index <= cutoff]\n",
    "target = target[target.index <= cutoff]\n",
    "print(f\"After truncation: {len(target):,} hours ({target.index.min()} to {target.index.max()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data validation: check for gaps / forward-fill artifacts ---\n",
    "# ENTSO-E had only Jan 2024 for NO_5 — the rest of 2024 was forward-filled\n",
    "# as a constant (47.5 EUR/MWh). Nord Pool has the real data.\n",
    "\n",
    "price_hourly_count = target.resample(\"D\").count()\n",
    "missing_days = price_hourly_count[price_hourly_count < 20]\n",
    "\n",
    "# Detect forward-fill artifacts: days where price never changes\n",
    "daily_unique = target.resample(\"D\").apply(lambda x: x.nunique())\n",
    "flat_days = daily_unique[daily_unique <= 1]\n",
    "\n",
    "print(\"Data quality check for price_eur_mwh:\")\n",
    "print(f\"  Total hours: {len(target):,}\")\n",
    "print(f\"  NaN count: {target.isna().sum()}\")\n",
    "print(f\"  Negative price hours: {(target < 0).sum()}\")\n",
    "print(f\"  Days with <20 hours coverage: {len(missing_days)}\")\n",
    "print(f\"  Days with constant price (ffill artifact): {len(flat_days)}\")\n",
    "\n",
    "if len(flat_days) > 5:\n",
    "    first_flat = flat_days.index[0]\n",
    "    last_flat = flat_days.index[-1]\n",
    "    print(f\"\\n  Forward-fill artifact detected: {first_flat.date()} to {last_flat.date()}\")\n",
    "    print(f\"  ({len(flat_days)} days of constant prices — likely ENTSO-E gap)\")\n",
    "    print(f\"  -> Will patch from hvakosterstrommen.no (Nord Pool) below\")\n",
    "\n",
    "# Year-by-year summary\n",
    "print(\"\\nYearly summary:\")\n",
    "for year in range(2022, 2026):\n",
    "    mask = target.index.year == year\n",
    "    yearly = target[mask]\n",
    "    n_unique = yearly.nunique()\n",
    "    print(f\"  {year}: {len(yearly):>5,} hours, {n_unique:>5,} unique vals, \"\n",
    "          f\"mean={yearly.mean():.1f}, std={yearly.std():.1f}, \"\n",
    "          f\"min={yearly.min():.1f}, max={yearly.max():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Patch: fill ENTSO-E price gaps with Nord Pool data ---\n",
    "# ENTSO-E only had Jan 2024 for NO_5 — Feb-Dec 2024 was forward-filled.\n",
    "# hvakosterstrommen.no (Nord Pool) has the actual day-ahead prices.\n",
    "\n",
    "from src.data.fetch_nordpool import fetch_prices as fetch_nordpool_prices\n",
    "\n",
    "# Load Nord Pool prices for the full period\n",
    "nordpool_all = fetch_nordpool_prices(\"2022-01-01\", \"2025-12-31\", cache=True)\n",
    "\n",
    "if ZONE in nordpool_all.columns:\n",
    "    nordpool_prices = nordpool_all[ZONE].rename(\"price_eur_mwh\")\n",
    "    \n",
    "    # Detect which hours in our data are forward-fill artifacts:\n",
    "    # consecutive hours with identical prices for 24+ hours straight\n",
    "    is_flat = (target.diff().abs() < 1e-6)\n",
    "    # Mark runs of 24+ identical values as suspect\n",
    "    flat_runs = is_flat.rolling(24, min_periods=24).sum()\n",
    "    suspect_mask = flat_runs >= 24\n",
    "    \n",
    "    # How many hours need patching?\n",
    "    n_suspect = suspect_mask.sum()\n",
    "    \n",
    "    if n_suspect > 100:\n",
    "        # Align Nord Pool to our index\n",
    "        nordpool_aligned = nordpool_prices.reindex(target.index)\n",
    "        \n",
    "        # Patch: replace suspect hours with Nord Pool prices where available\n",
    "        patched = target.copy()\n",
    "        can_patch = suspect_mask & nordpool_aligned.notna()\n",
    "        patched[can_patch] = nordpool_aligned[can_patch]\n",
    "        \n",
    "        n_patched = can_patch.sum()\n",
    "        print(f\"Patched {n_patched:,} hours of forward-fill artifacts with Nord Pool data\")\n",
    "        print(f\"  Before: {n_suspect:,} suspect hours (constant price runs)\")\n",
    "        \n",
    "        # Update target and df\n",
    "        target = patched\n",
    "        df[\"price_eur_mwh\"] = patched\n",
    "        \n",
    "        # Also recompute NOK prices (for reporting, not modeling)\n",
    "        if \"eur_nok\" in df.columns:\n",
    "            df[\"price_nok_mwh\"] = target * df[\"eur_nok\"]\n",
    "            df[\"price_nok_kwh\"] = df[\"price_nok_mwh\"] / 1000\n",
    "        \n",
    "        # Verify the fix\n",
    "        daily_unique_after = target.resample(\"D\").apply(lambda x: x.nunique())\n",
    "        flat_after = daily_unique_after[daily_unique_after <= 1]\n",
    "        print(f\"  After:  {len(flat_after)} days with constant prices\")\n",
    "        \n",
    "        # Show 2024 stats after fix\n",
    "        y_2024 = target[target.index.year == 2024]\n",
    "        print(f\"\\n  2024 after patch: {y_2024.nunique():,} unique values, \"\n",
    "              f\"mean={y_2024.mean():.1f}, std={y_2024.std():.1f}, \"\n",
    "              f\"min={y_2024.min():.1f}, max={y_2024.max():.1f}\")\n",
    "    else:\n",
    "        print(f\"Only {n_suspect} suspect hours detected — no patching needed\")\n",
    "else:\n",
    "    print(f\"Warning: {ZONE} not found in Nord Pool data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Validation / Test split\n",
    "TRAIN_END = pd.Timestamp(\"2024-12-31 23:00\", tz=\"Europe/Oslo\")\n",
    "VAL_END = pd.Timestamp(\"2025-06-30 23:00\", tz=\"Europe/Oslo\")\n",
    "\n",
    "y_train = target[target.index <= TRAIN_END]\n",
    "y_val = target[(target.index > TRAIN_END) & (target.index <= VAL_END)]\n",
    "y_test = target[target.index > VAL_END]\n",
    "\n",
    "# Also split the full DataFrame for features\n",
    "df_train = df[df.index <= TRAIN_END]\n",
    "df_val = df[(df.index > TRAIN_END) & (df.index <= VAL_END)]\n",
    "df_test = df[df.index > VAL_END]\n",
    "\n",
    "print(f\"Training:   {len(y_train):>6,} hours  ({y_train.index.min().date()} to {y_train.index.max().date()})\")\n",
    "print(f\"Validation: {len(y_val):>6,} hours  ({y_val.index.min().date()} to {y_val.index.max().date()})\")\n",
    "print(f\"Test:       {len(y_test):>6,} hours  ({y_test.index.min().date()} to {y_test.index.max().date()})\")\n",
    "print(f\"\\nTrain mean: {y_train.mean():.1f} EUR/MWh, std: {y_train.std():.1f}\")\n",
    "print(f\"Val mean:   {y_val.mean():.1f} EUR/MWh, std: {y_val.std():.1f}\")\n",
    "print(f\"Test mean:  {y_test.mean():.1f} EUR/MWh, std: {y_test.std():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the full price series with split boundaries\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "\n",
    "ax.plot(y_train.index, y_train, color=\"steelblue\", linewidth=0.5, label=\"Train\")\n",
    "ax.plot(y_val.index, y_val, color=\"darkorange\", linewidth=0.5, label=\"Validation\")\n",
    "ax.plot(y_test.index, y_test, color=\"green\", linewidth=0.5, label=\"Test\")\n",
    "\n",
    "ax.axvline(TRAIN_END, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Train/Val split\")\n",
    "ax.axvline(VAL_END, color=\"red\", linestyle=\"--\", alpha=0.7, label=\"Val/Test split\")\n",
    "\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"EUR/MWh\")\n",
    "ax.set_title(f\"Day-Ahead Electricity Price — {ZONE} (Bergen)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 1. Feature Overview\n",
    "\n",
    "Our feature matrix uses **fundamental features only** — no price lags. This is deliberate.\n",
    "\n",
    "**Why no price lags?** Features like `price_lag_24h` and `price_rolling_168h_mean` let the model\n",
    "learn \"price ≈ yesterday's price\" — a shortcut with r > 0.95 correlation to the target. While\n",
    "this gives low MAE on paper, the model isn't learning *why* prices move. By removing these,\n",
    "the model must learn from the **physical factors that actually drive electricity price**:\n",
    "production levels, consumption patterns, reservoir levels, gas prices, weather, and calendar effects.\n",
    "\n",
    "| Category | Features | Examples |\n",
    "|----------|----------|----------|\n",
    "| Calendar | 7 | hour_of_day, day_of_week, is_weekend, is_holiday |\n",
    "| Weather | ~5 | temperature, wind_speed, precipitation |\n",
    "| Commodities | 5 | ttf_gas_close, brent_oil_close, ng_fut_close |\n",
    "| Reservoir | 5 | reservoir_filling_pct, reservoir_vs_median |\n",
    "| FX | 1 | eur_nok |\n",
    "| ENTSO-E (load/gen) | ~15 | actual_load, generation_hydro, net_export |\n",
    "| Internal flows | ~6 | flow_from_no1, flow_from_no2, net_internal_flow |\n",
    "| Statnett | ~4 | net_exchange_mwh, production_mwh, consumption_mwh |\n",
    "\n",
    "**~42 fundamental features** — each one represents a real physical or economic driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature overview: grouped by category\n",
    "X_train_full, y_train_full = prepare_ml_features(df_train)\n",
    "X_val_full, y_val_full = prepare_ml_features(df_val)\n",
    "\n",
    "print(f\"Feature matrix: {X_train_full.shape[1]} features (after dropping NOK + price lag columns)\")\n",
    "print(f\"Training: {len(X_train_full):,} samples\")\n",
    "print(f\"Validation: {len(X_val_full):,} samples\")\n",
    "\n",
    "# Group features by category\n",
    "categories = {\n",
    "    \"Calendar\": [c for c in X_train_full.columns if c in [\n",
    "        \"hour_of_day\", \"day_of_week\", \"month\", \"week_of_year\",\n",
    "        \"is_weekend\", \"is_holiday\", \"is_business_hour\"]],\n",
    "    \"Weather\": [c for c in X_train_full.columns if c in [\n",
    "        \"temperature\", \"wind_speed\", \"precipitation\",\n",
    "        \"temperature_lag_24h\", \"temperature_rolling_24h_mean\"]],\n",
    "    \"Commodities\": [c for c in X_train_full.columns if any(\n",
    "        c.startswith(p) for p in [\"ttf_\", \"brent_\", \"coal_\", \"ng_\"])],\n",
    "    \"Reservoir\": [c for c in X_train_full.columns if \"reservoir\" in c],\n",
    "    \"FX\": [c for c in X_train_full.columns if c == \"eur_nok\"],\n",
    "    \"ENTSO-E\": [c for c in X_train_full.columns if any(\n",
    "        c.startswith(p) for p in [\"actual_\", \"load_\", \"generation_\",\n",
    "                                   \"hydro_\", \"wind_share\", \"total_net\",\n",
    "                                   \"n_cables\"])],\n",
    "    \"Internal Flows\": [c for c in X_train_full.columns if any(\n",
    "        c.startswith(p) for p in [\"flow_from_\", \"total_internal_\",\n",
    "                                   \"net_internal_\"])],\n",
    "    \"Statnett\": [c for c in X_train_full.columns if any(\n",
    "        c.startswith(p) for p in [\"net_exchange\", \"production_\", \"consumption_\", \"net_balance\"])],\n",
    "}\n",
    "\n",
    "# Also catch any uncategorized\n",
    "all_categorized = set()\n",
    "for cols in categories.values():\n",
    "    all_categorized.update(cols)\n",
    "uncategorized = [c for c in X_train_full.columns if c not in all_categorized]\n",
    "if uncategorized:\n",
    "    categories[\"Other\"] = uncategorized\n",
    "\n",
    "print(\"\\nFeatures by category (fundamentals only — no price lags):\")\n",
    "for cat, cols in categories.items():\n",
    "    if cols:\n",
    "        print(f\"  {cat} ({len(cols)}): {', '.join(cols[:5])}{'...' if len(cols) > 5 else ''}\")\n",
    "\n",
    "# Missing data summary\n",
    "missing_pct = X_train_full.isna().mean() * 100\n",
    "cols_with_missing = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
    "if len(cols_with_missing) > 0:\n",
    "    print(f\"\\nColumns with missing data ({len(cols_with_missing)}):\")\n",
    "    for col, pct in cols_with_missing.head(10).items():\n",
    "        print(f\"  {col}: {pct:.1f}%\")\n",
    "else:\n",
    "    print(\"\\nNo missing data in training features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap: top features vs price\n",
    "corr_with_price = X_train_full.corrwith(y_train_full).abs().sort_values(ascending=False)\n",
    "top_corr = corr_with_price.head(20)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_corr.plot(kind=\"barh\", ax=ax, color=\"steelblue\", alpha=0.8)\n",
    "ax.set_xlabel(\"Absolute Correlation with price_eur_mwh\")\n",
    "ax.set_title(f\"Top 20 Features Correlated with Price — {ZONE}\")\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 correlated features:\")\n",
    "for feat, corr in top_corr.head(10).items():\n",
    "    print(f\"  {feat}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 2. Naive Baseline\n",
    "\n",
    "The simplest possible forecast: predict that each hour's price equals the same hour from last week. This captures both daily and weekly patterns with zero modeling effort.\n",
    "\n",
    "**Every model below must beat this.** A model that can't beat `shift(168)` is adding complexity without value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll collect results for all methods here\n",
    "all_results = []\n",
    "all_forecasts = {}\n",
    "\n",
    "# Naive baseline: same hour last week\n",
    "naive = NaiveForecaster(name=\"Naive (same hour last week)\", horizon=len(y_val), frequency=\"h\", lag=168)\n",
    "naive.fit(y_train)\n",
    "naive_pred = naive.predict(steps=len(y_val))\n",
    "naive_pred.index = y_val.index\n",
    "\n",
    "naive_metrics = compute_metrics(y_val, naive_pred)\n",
    "all_results.append({\"name\": \"Naive (same hour last week)\", \"metrics\": naive_metrics, \"fit_time\": naive.fit_time_seconds})\n",
    "all_forecasts[\"Naive\"] = naive_pred\n",
    "\n",
    "print(\"Naive Baseline Results:\")\n",
    "for k, v in naive_metrics.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Convert to NOK/kWh for context\n",
    "latest_eur_nok = df[\"eur_nok\"].dropna().iloc[-1]\n",
    "print(f\"\\nIn NOK/kWh (EUR/NOK = {latest_eur_nok:.2f}):\")\n",
    "print(f\"  MAE: {naive_metrics['mae'] * latest_eur_nok / 1000:.3f} NOK/kWh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 3. Statistical Baselines (Summary)\n",
    "\n",
    "For reference, here are the best statistical method results from Phase 3.1. These methods are limited to 1–2 features (price history + optionally temperature/gas), so they can't leverage the full feature matrix.\n",
    "\n",
    "**Key limitation:** ARIMA/SARIMA treat future weather, reservoir levels, and load data as unavailable — they only use past price patterns. ML models use ALL features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick SARIMA baseline for reference (uses only price history)\n",
    "# We run just one statistical method as a reference point\n",
    "from src.models.forecasters import SARIMAXForecaster\n",
    "\n",
    "sarima = SARIMAXForecaster(\n",
    "    name=\"SARIMA (m=24)\", horizon=len(y_val), frequency=\"h\",\n",
    "    seasonal_period=24, max_train_size=4000,\n",
    ")\n",
    "sarima.fit(y_train)\n",
    "sarima_pred = sarima.predict(steps=len(y_val))\n",
    "sarima_pred.index = y_val.index\n",
    "\n",
    "sarima_metrics = compute_metrics(y_val, sarima_pred, naive_pred=naive_pred)\n",
    "all_results.append({\"name\": \"SARIMA (m=24)\", \"metrics\": sarima_metrics, \"fit_time\": sarima.fit_time_seconds})\n",
    "all_forecasts[\"SARIMA\"] = sarima_pred\n",
    "\n",
    "print(\"Statistical baseline — SARIMA:\")\n",
    "print(f\"  MAE: {sarima_metrics['mae']:.3f} EUR/MWh\")\n",
    "print(f\"  Skill score vs naive: {sarima_metrics.get('skill_score', 'N/A')}\")\n",
    "print(f\"\\n  Note: SARIMA uses ONLY price history — no weather, no gas prices, no reservoir data.\")\n",
    "print(f\"  ML models below will use all {X_train_full.shape[1]} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 4. XGBoost\n",
    "\n",
    "**XGBoost (eXtreme Gradient Boosting)** builds an ensemble of decision trees sequentially, where each new tree corrects the errors of the previous ones.\n",
    "\n",
    "**How it works:**\n",
    "1. Start with a simple prediction (e.g. mean price)\n",
    "2. Calculate the residuals (errors)\n",
    "3. Train a small decision tree to predict those residuals\n",
    "4. Add the tree's predictions (scaled by learning rate) to the running total\n",
    "5. Repeat for 1000 iterations (with early stopping if validation error stops improving)\n",
    "\n",
    "**Why it's good for electricity prices with fundamental features:**\n",
    "- Handles nonlinear relationships (e.g. temperature below 0°C has outsized effect on price)\n",
    "- Captures feature interactions (e.g. low wind + high gas price = spike)\n",
    "- Learns from the real price drivers: load, generation, reservoir levels, gas prices, weather\n",
    "- Built-in feature importance tells us which fundamental drivers matter most\n",
    "- Robust to missing data and different feature scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train XGBoost with early stopping on validation set\n",
    "xgb_model = MLPriceForecaster(\"xgboost\")\n",
    "xgb_model.fit(X_train_full, y_train_full, X_val_full, y_val_full)\n",
    "xgb_pred = xgb_model.predict(X_val_full)\n",
    "\n",
    "xgb_metrics = compute_metrics(y_val_full, xgb_pred, naive_pred=naive_pred)\n",
    "all_results.append({\"name\": \"XGBoost\", \"metrics\": xgb_metrics, \"fit_time\": xgb_model.fit_time_seconds})\n",
    "all_forecasts[\"XGBoost\"] = xgb_pred\n",
    "\n",
    "print(f\"XGBoost Results:\")\n",
    "for k, v in xgb_metrics.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(f\"\\n  Fit time: {xgb_model.fit_time_seconds:.1f}s\")\n",
    "print(f\"  vs Naive: {'BETTER' if xgb_metrics['mae'] < naive_metrics['mae'] else 'WORSE'} \"\n",
    "      f\"(skill_score: {xgb_metrics.get('skill_score', 'N/A')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost feature importance (top 20)\n",
    "xgb_importance = xgb_model.feature_importance()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "xgb_importance.head(20).plot(kind=\"barh\", ax=ax, color=\"steelblue\", alpha=0.8)\n",
    "ax.set_xlabel(\"Feature Importance (Gain)\")\n",
    "ax.set_title(\"XGBoost — Top 20 Features\")\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 features by importance:\")\n",
    "for i, (feat, imp) in enumerate(xgb_importance.head(10).items(), 1):\n",
    "    print(f\"  {i}. {feat}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 5. LightGBM\n",
    "\n",
    "**LightGBM** is Microsoft's gradient boosting framework. Key differences from XGBoost:\n",
    "\n",
    "- **Leaf-wise growth** (vs XGBoost's level-wise): grows the leaf that reduces error most, leading to deeper trees that converge faster\n",
    "- **Histogram-based splitting**: bins continuous features into discrete buckets for faster computation\n",
    "- **Typically 2-5x faster** than XGBoost for the same accuracy\n",
    "- **Native categorical support** (though we use numerical features here)\n",
    "\n",
    "In practice, LightGBM and XGBoost produce similar accuracy — the main advantage is speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train LightGBM with early stopping\n",
    "lgbm_model = MLPriceForecaster(\"lightgbm\")\n",
    "lgbm_model.fit(X_train_full, y_train_full, X_val_full, y_val_full)\n",
    "lgbm_pred = lgbm_model.predict(X_val_full)\n",
    "\n",
    "lgbm_metrics = compute_metrics(y_val_full, lgbm_pred, naive_pred=naive_pred)\n",
    "all_results.append({\"name\": \"LightGBM\", \"metrics\": lgbm_metrics, \"fit_time\": lgbm_model.fit_time_seconds})\n",
    "all_forecasts[\"LightGBM\"] = lgbm_pred\n",
    "\n",
    "print(f\"LightGBM Results:\")\n",
    "for k, v in lgbm_metrics.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(f\"\\n  Fit time: {lgbm_model.fit_time_seconds:.1f}s\")\n",
    "print(f\"  vs XGBoost: {'BETTER' if lgbm_metrics['mae'] < xgb_metrics['mae'] else 'WORSE or EQUAL'} \"\n",
    "      f\"({lgbm_model.fit_time_seconds:.1f}s vs {xgb_model.fit_time_seconds:.1f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 6. CatBoost\n",
    "\n",
    "**CatBoost** (Yandex) is designed for minimal tuning and handles categorical features natively.\n",
    "\n",
    "Key features:\n",
    "- **Ordered boosting**: uses a permutation-based approach that reduces overfitting (especially on small datasets)\n",
    "- **Symmetric trees**: builds balanced decision trees (all leaves at the same depth)\n",
    "- **Target encoding**: automatically converts categorical features using target statistics\n",
    "- **Typically needs less hyperparameter tuning** than XGBoost/LightGBM\n",
    "\n",
    "For tabular data with mixed feature types, CatBoost often works well out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train CatBoost with early stopping\n",
    "cat_model = MLPriceForecaster(\"catboost\")\n",
    "cat_model.fit(X_train_full, y_train_full, X_val_full, y_val_full)\n",
    "cat_pred = cat_model.predict(X_val_full)\n",
    "\n",
    "cat_metrics = compute_metrics(y_val_full, cat_pred, naive_pred=naive_pred)\n",
    "all_results.append({\"name\": \"CatBoost\", \"metrics\": cat_metrics, \"fit_time\": cat_model.fit_time_seconds})\n",
    "all_forecasts[\"CatBoost\"] = cat_pred\n",
    "\n",
    "print(f\"CatBoost Results:\")\n",
    "for k, v in cat_metrics.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(f\"\\n  Fit time: {cat_model.fit_time_seconds:.1f}s\")\n",
    "\n",
    "# Quick comparison\n",
    "print(f\"\\nML model comparison (MAE EUR/MWh):\")\n",
    "print(f\"  XGBoost:  {xgb_metrics['mae']:.3f}\")\n",
    "print(f\"  LightGBM: {lgbm_metrics['mae']:.3f}\")\n",
    "print(f\"  CatBoost: {cat_metrics['mae']:.3f}\")\n",
    "print(f\"  Naive:    {naive_metrics['mae']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 7. Ensemble\n",
    "\n",
    "**Why ensembles work:** Different models make different mistakes. By averaging their predictions, individual errors tend to cancel out. This is one of the most reliable ways to improve accuracy in ML.\n",
    "\n",
    "We build two ensembles:\n",
    "1. **Simple average** — equal weight to each model\n",
    "2. **Inverse-MAE weighted** — better models get more weight\n",
    "\n",
    "Ensembles almost always match or beat the best individual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Build ensemble using all 3 models (already trained above)\n",
    "# Compute inverse-MAE weights\n",
    "model_maes = {\n",
    "    \"xgboost\": xgb_metrics[\"mae\"],\n",
    "    \"lightgbm\": lgbm_metrics[\"mae\"],\n",
    "    \"catboost\": cat_metrics[\"mae\"],\n",
    "}\n",
    "inv_maes = {k: 1.0 / v for k, v in model_maes.items()}\n",
    "total_inv = sum(inv_maes.values())\n",
    "weights = {k: v / total_inv for k, v in inv_maes.items()}\n",
    "\n",
    "print(\"Ensemble weights (inverse-MAE):\")\n",
    "for k, w in weights.items():\n",
    "    print(f\"  {k}: {w:.3f} (MAE: {model_maes[k]:.3f})\")\n",
    "\n",
    "# Weighted ensemble\n",
    "ensemble_pred = (\n",
    "    weights[\"xgboost\"] * xgb_pred\n",
    "    + weights[\"lightgbm\"] * lgbm_pred\n",
    "    + weights[\"catboost\"] * cat_pred\n",
    ")\n",
    "ensemble_pred.name = \"Ensemble (weighted)\"\n",
    "\n",
    "# Simple average\n",
    "simple_avg_pred = (xgb_pred + lgbm_pred + cat_pred) / 3\n",
    "simple_avg_pred.name = \"Ensemble (simple avg)\"\n",
    "\n",
    "# Evaluate both\n",
    "ens_w_metrics = compute_metrics(y_val_full, ensemble_pred, naive_pred=naive_pred)\n",
    "ens_s_metrics = compute_metrics(y_val_full, simple_avg_pred, naive_pred=naive_pred)\n",
    "\n",
    "all_results.append({\"name\": \"Ensemble (weighted)\", \"metrics\": ens_w_metrics, \"fit_time\": 0})\n",
    "all_results.append({\"name\": \"Ensemble (simple avg)\", \"metrics\": ens_s_metrics, \"fit_time\": 0})\n",
    "all_forecasts[\"Ensemble (weighted)\"] = ensemble_pred\n",
    "all_forecasts[\"Ensemble (simple avg)\"] = simple_avg_pred\n",
    "\n",
    "print(f\"\\nEnsemble Results:\")\n",
    "print(f\"  Weighted:   MAE={ens_w_metrics['mae']:.3f}, skill_score={ens_w_metrics.get('skill_score', 'N/A')}\")\n",
    "print(f\"  Simple avg: MAE={ens_s_metrics['mae']:.3f}, skill_score={ens_s_metrics.get('skill_score', 'N/A')}\")\n",
    "print(f\"  Best individual: {min(model_maes, key=model_maes.get)} MAE={min(model_maes.values()):.3f}\")\n",
    "print(f\"\\n  Ensemble {'beats' if ens_w_metrics['mae'] <= min(model_maes.values()) else 'does not beat'} best individual model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast overlay: first 2 weeks of validation\n",
    "two_weeks = y_val_full.index[:336]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "ax.plot(y_val_full.loc[two_weeks].index, y_val_full.loc[two_weeks],\n",
    "        color=\"black\", linewidth=1.2, label=\"Actual\", zorder=5)\n",
    "ax.plot(xgb_pred.loc[two_weeks].index, xgb_pred.loc[two_weeks],\n",
    "        color=\"tab:blue\", linewidth=0.8, alpha=0.7, label=f\"XGBoost (MAE={xgb_metrics['mae']:.1f})\")\n",
    "ax.plot(lgbm_pred.loc[two_weeks].index, lgbm_pred.loc[two_weeks],\n",
    "        color=\"tab:orange\", linewidth=0.8, alpha=0.7, label=f\"LightGBM (MAE={lgbm_metrics['mae']:.1f})\")\n",
    "ax.plot(cat_pred.loc[two_weeks].index, cat_pred.loc[two_weeks],\n",
    "        color=\"tab:green\", linewidth=0.8, alpha=0.7, label=f\"CatBoost (MAE={cat_metrics['mae']:.1f})\")\n",
    "ax.plot(ensemble_pred.loc[two_weeks].index, ensemble_pred.loc[two_weeks],\n",
    "        color=\"tab:red\", linewidth=1.0, alpha=0.9, label=f\"Ensemble (MAE={ens_w_metrics['mae']:.1f})\")\n",
    "\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"EUR/MWh\")\n",
    "ax.set_title(f\"ML Model Predictions — First 2 Weeks of Validation — {ZONE}\")\n",
    "ax.legend(loc=\"upper right\", fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 8. Walk-Forward Validation\n",
    "\n",
    "**Why walk-forward?** A single train/val split can be misleading — the model might perform well in January but poorly in July. Walk-forward validation simulates real deployment:\n",
    "\n",
    "```\n",
    "Fold 1: Train [2022-01 → 2025-01] | Val [2025-01 → 2025-02]\n",
    "Fold 2: Train [2022-01 → 2025-02] | Val [2025-02 → 2025-03]\n",
    "Fold 3: Train [2022-01 → 2025-03] | Val [2025-03 → 2025-04]\n",
    "...expanding window, always predicting unseen future data\n",
    "```\n",
    "\n",
    "This tells us: **is the model consistently good, or did we get lucky with one split?**\n",
    "\n",
    "**Important:** We never use random k-fold cross-validation for time series — it leaks future information into training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Walk-forward validation with expanding window\n",
    "# Use the combined train+val data for walk-forward\n",
    "df_walkforward = pd.concat([df_train, df_val])\n",
    "\n",
    "# Use the best-performing individual model type\n",
    "best_model_type = min(model_maes, key=model_maes.get)\n",
    "print(f\"Walk-forward validation using: {best_model_type}\")\n",
    "print(f\"Data: {len(df_walkforward):,} hours\")\n",
    "print(f\"Configuration: 6 folds, ~720 hours (~1 month) each\\n\")\n",
    "\n",
    "wf_results = walk_forward_validate(\n",
    "    df_walkforward,\n",
    "    model_type=best_model_type,\n",
    "    n_splits=6,\n",
    "    val_size_hours=720,\n",
    "    target_col=\"price_eur_mwh\",\n",
    ")\n",
    "\n",
    "# Display per-fold metrics\n",
    "wf_rows = []\n",
    "for r in wf_results:\n",
    "    row = {\n",
    "        \"Fold\": r[\"fold\"],\n",
    "        \"Train Size\": f\"{r['train_size']:,}\",\n",
    "        \"Val Period\": f\"{r['val_start'].strftime('%Y-%m-%d')} → {r['val_end'].strftime('%Y-%m-%d')}\",\n",
    "        \"MAE\": r[\"metrics\"].get(\"mae\", np.nan),\n",
    "        \"RMSE\": r[\"metrics\"].get(\"rmse\", np.nan),\n",
    "        \"Skill Score\": r[\"metrics\"].get(\"skill_score\", np.nan),\n",
    "        \"Dir. Acc.\": r[\"metrics\"].get(\"directional_accuracy\", np.nan),\n",
    "        \"Fit Time (s)\": r[\"fit_time\"],\n",
    "    }\n",
    "    wf_rows.append(row)\n",
    "\n",
    "wf_df = pd.DataFrame(wf_rows)\n",
    "display(wf_df)\n",
    "\n",
    "# Summary statistics\n",
    "mae_values = [r[\"metrics\"][\"mae\"] for r in wf_results if \"mae\" in r[\"metrics\"]]\n",
    "print(f\"\\nWalk-forward MAE summary:\")\n",
    "print(f\"  Mean: {np.mean(mae_values):.3f} EUR/MWh\")\n",
    "print(f\"  Std:  {np.std(mae_values):.3f} EUR/MWh\")\n",
    "print(f\"  Min:  {np.min(mae_values):.3f} (best fold)\")\n",
    "print(f\"  Max:  {np.max(mae_values):.3f} (worst fold)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk-forward stability plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MAE per fold\n",
    "ax = axes[0]\n",
    "folds = [r[\"fold\"] for r in wf_results]\n",
    "maes = [r[\"metrics\"][\"mae\"] for r in wf_results]\n",
    "skills = [r[\"metrics\"].get(\"skill_score\", 0) for r in wf_results]\n",
    "\n",
    "ax.bar(folds, maes, color=\"steelblue\", alpha=0.8)\n",
    "ax.axhline(np.mean(maes), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(maes):.1f}\")\n",
    "ax.set_xlabel(\"Fold\")\n",
    "ax.set_ylabel(\"MAE (EUR/MWh)\")\n",
    "ax.set_title(\"Walk-Forward MAE by Fold\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Skill score per fold\n",
    "ax = axes[1]\n",
    "colors = [\"green\" if s > 0 else \"red\" for s in skills]\n",
    "ax.bar(folds, skills, color=colors, alpha=0.8)\n",
    "ax.axhline(0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "ax.set_xlabel(\"Fold\")\n",
    "ax.set_ylabel(\"Skill Score vs Naive\")\n",
    "ax.set_title(\"Walk-Forward Skill Score by Fold\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 9. SHAP Analysis\n",
    "\n",
    "**SHAP (SHapley Additive exPlanations)** goes beyond simple feature importance. For each prediction, SHAP shows **how much each feature pushed the prediction up or down** from the baseline.\n",
    "\n",
    "This is critical for understanding:\n",
    "- Does the model make **physical sense**? (e.g., high temperature in winter should decrease price via less heating demand)\n",
    "- Are there **surprising relationships**? (features we didn't expect to matter)\n",
    "- Is the model **trustworthy** for deployment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import shap\n",
    "\n",
    "# Use the XGBoost model for SHAP (best native support)\n",
    "print(\"Computing SHAP values for XGBoost model...\")\n",
    "print(f\"Using {len(X_val_full)} validation samples\\n\")\n",
    "\n",
    "# Use a subsample for speed (SHAP on full val set can be slow)\n",
    "shap_sample_size = min(2000, len(X_val_full))\n",
    "X_shap = X_val_full.ffill().bfill().fillna(0).iloc[:shap_sample_size]\n",
    "\n",
    "explainer = shap.TreeExplainer(xgb_model.model_)\n",
    "shap_values = explainer.shap_values(X_shap)\n",
    "\n",
    "print(f\"SHAP values computed: {shap_values.shape}\")\n",
    "print(f\"Base value (expected prediction): {explainer.expected_value:.2f} EUR/MWh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP summary plot — global feature importance with direction\n",
    "# Each dot is one prediction. Position shows SHAP value (impact on output).\n",
    "# Color shows feature value (red=high, blue=low).\n",
    "plt.figure(figsize=(10, 10))\n",
    "shap.summary_plot(shap_values, X_shap, max_display=20, show=False)\n",
    "plt.title(f\"SHAP Feature Importance — XGBoost — {ZONE}\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mean absolute SHAP values (global importance)\n",
    "mean_abs_shap = pd.Series(\n",
    "    np.abs(shap_values).mean(axis=0),\n",
    "    index=X_shap.columns,\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 features by mean |SHAP value|:\")\n",
    "for i, (feat, val) in enumerate(mean_abs_shap.head(10).items(), 1):\n",
    "    print(f\"  {i}. {feat}: {val:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP dependence plots for top 5 features\n",
    "# Shows how each feature value affects the prediction\n",
    "top_5_features = mean_abs_shap.head(5).index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for i, feat in enumerate(top_5_features):\n",
    "    ax = axes[i]\n",
    "    feat_idx = list(X_shap.columns).index(feat)\n",
    "    ax.scatter(X_shap[feat].values, shap_values[:, feat_idx],\n",
    "               alpha=0.3, s=5, color=\"steelblue\")\n",
    "    ax.set_xlabel(feat, fontsize=9)\n",
    "    ax.set_ylabel(\"SHAP value\" if i == 0 else \"\")\n",
    "    ax.axhline(0, color=\"gray\", linewidth=0.5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle(\"SHAP Dependence — Top 5 Features\", fontsize=13, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP waterfall for a single prediction (most expensive hour in validation)\n",
    "# Shows how each feature contributed to this specific prediction\n",
    "max_price_idx = y_val_full.idxmax()\n",
    "sample_idx = X_shap.index.get_indexer([max_price_idx], method=\"nearest\")[0]\n",
    "\n",
    "print(f\"Explaining the highest-price prediction in validation:\")\n",
    "print(f\"  Timestamp: {X_shap.index[sample_idx]}\")\n",
    "print(f\"  Actual price: {y_val_full.iloc[sample_idx]:.1f} EUR/MWh\")\n",
    "print(f\"  Predicted: {xgb_pred.iloc[sample_idx]:.1f} EUR/MWh\\n\")\n",
    "\n",
    "shap.initjs()\n",
    "explanation = shap.Explanation(\n",
    "    values=shap_values[sample_idx],\n",
    "    base_values=explainer.expected_value,\n",
    "    data=X_shap.iloc[sample_idx],\n",
    "    feature_names=list(X_shap.columns),\n",
    ")\n",
    "shap.plots.waterfall(explanation, max_display=15, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## 10. Yr Weather Forecast Integration\n",
    "\n",
    "So far, all models used **historical weather observations** — actual temperature, wind, and precipitation that were measured at the time. For a real forecast, we need **future weather** data.\n",
    "\n",
    "**Yr Locationforecast** (MET Norway) provides ~9 days of weather forecasts. By replacing historical weather columns with Yr forecast values, we can make **genuinely forward-looking** price predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.fetch_yr_forecast import fetch_yr_forecast\n",
    "\n",
    "# Fetch Yr weather forecast for Bergen (NO_5)\n",
    "yr_df = fetch_yr_forecast(ZONE, cache=True)\n",
    "\n",
    "if not yr_df.empty:\n",
    "    print(f\"Yr forecast for {ZONE}:\")\n",
    "    print(f\"  Hours: {len(yr_df)}\")\n",
    "    print(f\"  Range: {yr_df.index.min()} to {yr_df.index.max()}\")\n",
    "    print(f\"  Columns: {list(yr_df.columns)}\")\n",
    "    print(f\"\\nForecast summary:\")\n",
    "    for col in ['yr_temperature', 'yr_wind_speed', 'yr_precipitation_1h', 'yr_cloud_cover']:\n",
    "        if col in yr_df.columns:\n",
    "            print(f\"  {col}: mean={yr_df[col].mean():.1f}, \"\n",
    "                  f\"min={yr_df[col].min():.1f}, max={yr_df[col].max():.1f}\")\n",
    "    \n",
    "    # Visualize the forecast\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 8))\n",
    "    \n",
    "    for i, (col, title, unit) in enumerate([\n",
    "        ('yr_temperature', 'Temperature Forecast', '°C'),\n",
    "        ('yr_wind_speed', 'Wind Speed Forecast', 'm/s'),\n",
    "        ('yr_precipitation_1h', 'Precipitation Forecast', 'mm/h'),\n",
    "        ('yr_cloud_cover', 'Cloud Cover Forecast', '%'),\n",
    "    ]):\n",
    "        ax = axes[i // 2, i % 2]\n",
    "        if col in yr_df.columns:\n",
    "            ax.plot(yr_df.index, yr_df[col], color='steelblue', linewidth=1)\n",
    "            ax.set_ylabel(unit)\n",
    "            ax.set_title(title)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    fig.suptitle(f'Yr Weather Forecast — {ZONE} (Bergen)', fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Yr forecast fetch failed — check network connection.\")\n",
    "    print(\"Forward price forecast will be skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward price forecast using Yr weather + best ML model\n",
    "if not yr_df.empty:\n",
    "    # Use the ensemble's best model for the forward forecast\n",
    "    # Use last 168+ rows of feature data for lag computation\n",
    "    last_features = df.iloc[-336:]  # 2 weeks of history for lags\n",
    "    \n",
    "    print(f\"Forward forecast using {best_model_type} model + Yr weather\")\n",
    "    print(f\"Forecast from: {yr_df.index.min()}\")\n",
    "    print(f\"Forecast to:   {yr_df.index.max()}\")\n",
    "    print(f\"EUR/NOK rate:  {latest_eur_nok:.4f}\\n\")\n",
    "    \n",
    "    # Pick the best individual model\n",
    "    best_ml_model = {\"xgboost\": xgb_model, \"lightgbm\": lgbm_model, \"catboost\": cat_model}[best_model_type]\n",
    "    \n",
    "    forward_forecast = forecast_with_yr(\n",
    "        model=best_ml_model,\n",
    "        last_features=last_features,\n",
    "        yr_forecast_df=yr_df,\n",
    "        eur_nok=latest_eur_nok,\n",
    "    )\n",
    "    \n",
    "    if not forward_forecast.empty:\n",
    "        print(f\"Hourly forecast: {len(forward_forecast)} hours\")\n",
    "        print(f\"  EUR/MWh: mean={forward_forecast['price_eur_mwh'].mean():.1f}, \"\n",
    "              f\"min={forward_forecast['price_eur_mwh'].min():.1f}, \"\n",
    "              f\"max={forward_forecast['price_eur_mwh'].max():.1f}\")\n",
    "        print(f\"  NOK/kWh: mean={forward_forecast['price_nok_kwh'].mean():.3f}, \"\n",
    "              f\"min={forward_forecast['price_nok_kwh'].min():.3f}, \"\n",
    "              f\"max={forward_forecast['price_nok_kwh'].max():.3f}\")\n",
    "        \n",
    "        # Aggregate hourly forecast to daily\n",
    "        forward_daily = forward_forecast.resample(\"D\").agg({\n",
    "            \"price_eur_mwh\": [\"mean\", \"min\", \"max\"],\n",
    "            \"price_nok_kwh\": [\"mean\", \"min\", \"max\"],\n",
    "        })\n",
    "        forward_daily.columns = [f\"{col}_{agg}\" for col, agg in forward_daily.columns]\n",
    "        forward_daily = forward_daily[forward_daily[\"price_eur_mwh_mean\"].notna()]\n",
    "        \n",
    "        print(f\"\\nDaily aggregation: {len(forward_daily)} days\")\n",
    "        display(forward_daily.round(3))\n",
    "else:\n",
    "    forward_forecast = pd.DataFrame()\n",
    "    forward_daily = pd.DataFrame()\n",
    "    print(\"Skipping forward forecast (no Yr data).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "## 11. Daily Price Forecast (NOK/kWh)\n",
    "\n",
    "The forward forecast from the ML model + Yr weather, aggregated to **daily resolution** and displayed in **NOK/kWh** (the unit Norwegian consumers care about).\n",
    "\n",
    "Hourly predictions are aggregated per day showing:\n",
    "- **Mean** — average price across all hours (bar height)\n",
    "- **Min–Max** — cheapest and most expensive hour (shaded range)\n",
    "\n",
    "Three horizons:\n",
    "- **Day-ahead (1 day):** Most accurate — matches Nord Pool auction timeline\n",
    "- **Week-ahead (7 days):** Planning horizon\n",
    "- **Full Yr range (~9 days):** Maximum available forecast horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": "if not forward_daily.empty:\n    # Daily bar chart with min–max range (NOK/kWh)\n    fig = go.Figure()\n\n    # Min–max range as shaded area\n    fig.add_trace(go.Scatter(\n        x=forward_daily.index,\n        y=forward_daily[\"price_nok_kwh_max\"],\n        mode=\"lines\",\n        line=dict(width=0),\n        showlegend=False,\n        hoverinfo=\"skip\",\n    ))\n    fig.add_trace(go.Scatter(\n        x=forward_daily.index,\n        y=forward_daily[\"price_nok_kwh_min\"],\n        mode=\"lines\",\n        line=dict(width=0),\n        fill=\"tonexty\",\n        fillcolor=\"rgba(255, 165, 0, 0.2)\",\n        name=\"Hourly min–max range\",\n        hoverinfo=\"skip\",\n    ))\n\n    # Day-ahead (first day) in red, rest in orange\n    colors = [\"#d62728\"] + [\"#ff7f0e\"] * (len(forward_daily) - 1)\n    labels = [\"Day-ahead\"] + [\"\"] * (len(forward_daily) - 1)\n\n    fig.add_trace(go.Bar(\n        x=forward_daily.index,\n        y=forward_daily[\"price_nok_kwh_mean\"],\n        marker_color=colors,\n        name=\"Daily mean price\",\n        text=[f\"{v:.3f}\" for v in forward_daily[\"price_nok_kwh_mean\"]],\n        textposition=\"outside\",\n        hovertemplate=(\n            \"%{x|%a %d %b}<br>\"\n            \"Mean: %{y:.3f} NOK/kWh<br>\"\n            \"Min: %{customdata[0]:.3f}<br>\"\n            \"Max: %{customdata[1]:.3f}<extra></extra>\"\n        ),\n        customdata=forward_daily[[\"price_nok_kwh_min\", \"price_nok_kwh_max\"]].values,\n    ))\n\n    # Reference: last known price (use add_shape + add_annotation instead of\n    # add_hline with annotation_text to avoid Plotly bug with tz-aware Timestamps)\n    last_known_price_nok = target.iloc[-1] * latest_eur_nok / 1000\n    fig.add_shape(\n        type=\"line\",\n        x0=0, x1=1, xref=\"paper\",\n        y0=last_known_price_nok, y1=last_known_price_nok,\n        line=dict(color=\"black\", width=1, dash=\"dash\"),\n        opacity=0.5,\n    )\n    fig.add_annotation(\n        x=0.0, xref=\"paper\",\n        y=last_known_price_nok,\n        text=f\"Last known: {last_known_price_nok:.3f} NOK/kWh ({target.index[-1].strftime('%Y-%m-%d')})\",\n        showarrow=False,\n        font=dict(size=10),\n        xanchor=\"left\",\n        yanchor=\"bottom\",\n    )\n\n    fig.update_layout(\n        title=f\"Daily Price Forecast — {ZONE} (Bergen) — {best_model_type.upper()} + Yr Weather\",\n        xaxis_title=\"Date\",\n        yaxis_title=\"NOK/kWh\",\n        hovermode=\"x unified\",\n        height=500,\n        showlegend=True,\n        bargap=0.15,\n    )\n    fig.show()\n\n    # Summary table by horizon (daily)\n    n_days = len(forward_daily)\n    horizons = {\n        \"Day-ahead (1 day)\": forward_daily.iloc[:1],\n        f\"Week-ahead (7 days)\": forward_daily.iloc[:min(7, n_days)],\n        f\"Full range ({n_days} days)\": forward_daily,\n    }\n\n    print(f\"\\nForecast summary (EUR/NOK = {latest_eur_nok:.2f}):\")\n    print(f\"{'Horizon':<22} {'Mean NOK/kWh':>12} {'Min':>8} {'Max':>8} {'Mean EUR/MWh':>13}\")\n    print(\"-\" * 67)\n    for name, data in horizons.items():\n        print(f\"{name:<22} {data['price_nok_kwh_mean'].mean():>12.3f} \"\n              f\"{data['price_nok_kwh_min'].min():>8.3f} {data['price_nok_kwh_max'].max():>8.3f} \"\n              f\"{data['price_eur_mwh_mean'].mean():>13.1f}\")\nelse:\n    print(\"No forward forecast available (Yr data was not fetched).\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "## 12. Model Comparison — Grand Table\n",
    "\n",
    "All methods compared: Naive, SARIMA (statistical), XGBoost, LightGBM, CatBoost, and both ensembles. Metrics in EUR/MWh with NOK/kWh equivalents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build grand comparison table\n",
    "comp = comparison_table(all_results)\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(f\"MODEL COMPARISON — {ZONE} (Bergen) — Validation Period\")\n",
    "print(f\"Val period: {y_val.index.min().date()} to {y_val.index.max().date()}\")\n",
    "print(\"=\" * 90)\n",
    "display(comp)\n",
    "\n",
    "# Add NOK/kWh column\n",
    "print(f\"\\nNOK/kWh equivalent (EUR/NOK = {latest_eur_nok:.2f}):\")\n",
    "for _, row in comp.iterrows():\n",
    "    mae_nok = row['mae'] * latest_eur_nok / 1000\n",
    "    print(f\"  {row['Method']:30s}: MAE = {mae_nok:.3f} NOK/kWh  ({row['mae']:.1f} EUR/MWh)\")\n",
    "\n",
    "# Highlight winners\n",
    "naive_mae = naive_metrics[\"mae\"]\n",
    "print(f\"\\nNaive baseline MAE: {naive_mae:.1f} EUR/MWh\")\n",
    "beat_naive = comp[comp[\"mae\"] < naive_mae]\n",
    "if len(beat_naive) > 0:\n",
    "    print(f\"{len(beat_naive)} method(s) beat the naive baseline:\")\n",
    "    for _, row in beat_naive.iterrows():\n",
    "        improvement = (1 - row['mae'] / naive_mae) * 100\n",
    "        print(f\"  {row['Method']}: {improvement:.1f}% better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "methods = comp[\"Method\"].values\n",
    "maes = comp[\"mae\"].values\n",
    "colors = [\"tab:green\" if m < naive_mae else \"tab:red\" for m in maes]\n",
    "\n",
    "bars = ax.barh(range(len(methods)), maes, color=colors, alpha=0.8)\n",
    "ax.set_yticks(range(len(methods)))\n",
    "ax.set_yticklabels(methods)\n",
    "ax.axvline(naive_mae, color=\"black\", linestyle=\"--\", linewidth=1.5,\n",
    "           label=f\"Naive baseline ({naive_mae:.1f})\")\n",
    "ax.set_xlabel(\"MAE (EUR/MWh) — lower is better\")\n",
    "ax.set_title(f\"Forecast MAE Comparison — {ZONE}\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "\n",
    "for i, (method, mae) in enumerate(zip(methods, maes)):\n",
    "    ax.text(mae + 0.3, i, f\"{mae:.1f}\", va=\"center\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "## 13. Residual Analysis (Ensemble)\n",
    "\n",
    "A good model's residuals (errors) should look like white noise:\n",
    "- **Normally distributed** — errors are random, not systematic\n",
    "- **No autocorrelation** — the model captured all temporal patterns\n",
    "- **No structure by hour or month** — works equally well at all times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ensemble predictions for residual analysis\n",
    "best_ensemble_pred = ensemble_pred\n",
    "residuals = (y_val_full - best_ensemble_pred).dropna()\n",
    "\n",
    "# 3-panel diagnostic\n",
    "fig = plot_residuals(y_val_full, best_ensemble_pred, method_name=\"Ensemble (weighted)\")\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk test\n",
    "sample_resid = residuals.sample(min(2000, len(residuals)), random_state=42)\n",
    "sw_stat, sw_p = stats.shapiro(sample_resid)\n",
    "print(f\"\\nShapiro-Wilk test: stat={sw_stat:.4f}, p={sw_p:.6f}\")\n",
    "print(f\"Residuals {'appear' if sw_p > 0.05 else 'do NOT appear'} normally distributed\")\n",
    "print(f\"\\nResidual stats: mean={residuals.mean():.2f}, std={residuals.std():.2f}\")\n",
    "print(f\"Skewness: {residuals.skew():.3f}, Kurtosis: {residuals.kurtosis():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals by hour-of-day and by month\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# By hour\n",
    "ax = axes[0]\n",
    "hourly_mae = residuals.abs().groupby(residuals.index.hour).mean()\n",
    "ax.bar(hourly_mae.index, hourly_mae.values, color=\"steelblue\", alpha=0.8)\n",
    "ax.axhline(residuals.abs().mean(), color=\"red\", linestyle=\"--\", label=f\"Overall MAE: {residuals.abs().mean():.1f}\")\n",
    "ax.set_xlabel(\"Hour of Day\")\n",
    "ax.set_ylabel(\"MAE (EUR/MWh)\")\n",
    "ax.set_title(\"Residual MAE by Hour of Day\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# By month\n",
    "ax = axes[1]\n",
    "monthly_mae = residuals.abs().groupby(residuals.index.month).mean()\n",
    "ax.bar(monthly_mae.index, monthly_mae.values, color=\"darkorange\", alpha=0.8)\n",
    "ax.axhline(residuals.abs().mean(), color=\"red\", linestyle=\"--\", label=f\"Overall MAE: {residuals.abs().mean():.1f}\")\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"MAE (EUR/MWh)\")\n",
    "ax.set_title(\"Residual MAE by Month\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Rolling MAE\n",
    "fig, ax = plt.subplots(figsize=(16, 4))\n",
    "rolling_mae = residuals.abs().rolling(168).mean()  # 7-day window\n",
    "ax.plot(rolling_mae.index, rolling_mae, color=\"darkorange\", linewidth=1)\n",
    "ax.axhline(residuals.abs().mean(), color=\"red\", linestyle=\"--\", linewidth=0.8)\n",
    "ax.set_ylabel(\"7-Day Rolling MAE (EUR/MWh)\")\n",
    "ax.set_title(\"Error Stability Over Time — Ensemble\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "## 14. Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 80)\n",
    "print(f\"PHASE 3 SUMMARY — ML Price Forecasting for {ZONE} (Bergen)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nValidation period: {y_val.index.min().date()} to {y_val.index.max().date()}\")\n",
    "print(f\"Actual price range: {y_val.min():.1f} to {y_val.max():.1f} EUR/MWh\")\n",
    "print(f\"Features used: {X_train_full.shape[1]} (fundamentals only — no price lags)\")\n",
    "\n",
    "print(f\"\\n--- Final Rankings ---\")\n",
    "display(comp)\n",
    "\n",
    "# Key numbers\n",
    "best_method = comp.iloc[0]['Method']\n",
    "best_mae = comp.iloc[0]['mae']\n",
    "best_mae_nok = best_mae * latest_eur_nok / 1000\n",
    "\n",
    "print(f\"\\n--- Key Results ---\")\n",
    "print(f\"Best model: {best_method}\")\n",
    "print(f\"  MAE: {best_mae:.1f} EUR/MWh ({best_mae_nok:.3f} NOK/kWh)\")\n",
    "print(f\"  vs Naive ({naive_mae:.1f} EUR/MWh): \"\n",
    "      f\"{(1 - best_mae/naive_mae)*100:.1f}% improvement\")\n",
    "\n",
    "print(f\"\\n--- Top Features (SHAP) ---\")\n",
    "for i, (feat, val) in enumerate(mean_abs_shap.head(5).items(), 1):\n",
    "    print(f\"  {i}. {feat} (SHAP: {val:.3f})\")\n",
    "\n",
    "if not forward_forecast.empty:\n",
    "    print(f\"\\n--- Forward Forecast ---\")\n",
    "    print(f\"  Horizon: {len(forward_forecast)} hours ({forward_forecast.index.min().date()} to {forward_forecast.index.max().date()})\")\n",
    "    print(f\"  Mean: {forward_forecast['price_nok_kwh'].mean():.3f} NOK/kWh\")\n",
    "    print(f\"  Range: {forward_forecast['price_nok_kwh'].min():.3f} – {forward_forecast['price_nok_kwh'].max():.3f} NOK/kWh\")\n",
    "\n",
    "print(f\"\\n--- Walk-Forward Validation ({best_model_type}) ---\")\n",
    "print(f\"  {len(wf_results)} folds, MAE: {np.mean(mae_values):.1f} +/- {np.std(mae_values):.1f} EUR/MWh\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Observations:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"1. Fundamentals-only approach: no price lag features. The model learns from\")\n",
    "print(\"   actual supply/demand drivers (load, generation, reservoirs, gas, weather)\")\n",
    "print(\"   instead of the autoregressive shortcut 'price ~ yesterday's price'.\")\n",
    "print(\"2. SHAP analysis reveals which physical/economic drivers matter most for NO_5.\")\n",
    "print(\"3. The ensemble combines XGBoost + LightGBM + CatBoost for more robust predictions.\")\n",
    "print(\"4. Yr weather forecasts enable genuinely forward-looking predictions.\")\n",
    "print(\"5. Walk-forward validation confirms model stability across time periods.\")\n",
    "print(\"\")\n",
    "print(\"Next steps:\")\n",
    "print(\"- Replicate for other zones (NO_1-NO_4)\")\n",
    "print(\"- Optuna hyperparameter tuning (Phase 4)\")\n",
    "print(\"- Multi-target forecasting: reservoir, demand, production (Phase 5)\")\n",
    "print(\"- Streamlit dashboard with live Yr integration (Phase 7)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}